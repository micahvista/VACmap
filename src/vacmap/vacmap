# VACmap - a long-read aligner for structural variation discovery
# Copyright (C) 2023 Hongyu Ding
# Refactored with argparse and debugged

import sys
import os
import time
import argparse
import logging
import multiprocessing
import copy
import re
import gzip
import glob
from io import StringIO
from multiprocessing import Manager

# Third-party imports
import edlib
import pandas as pd
import pysam
import numpy as np
import heapq
from numba import njit, jit
from numba.typed import Dict as NumbaDict
from numba.typed import List as NumbaList
from Bio import SeqIO
from Bio.Seq import Seq
import matplotlib.pyplot as plt

# Local/Custom imports
# Note: Ensure these are in your PYTHONPATH
import vacmap_index as mp
import vacmap.output_functions as output_functions

# Logging setup
logging.basicConfig(
    format='%(levelname)s: %(asctime)s %(message)s',
    datefmt='%m/%d/%Y %I:%M:%S %p',
    level=logging.INFO
)

# Hardcoded test sequence from original code
TEST_SEQ = 'AATTTTCTGCTTAGTTTTGTTGTTGCATCTTCAAACAGCAGCGTTTGGCCGCAGAGCATGGCAAGTGCTTAAGAGAAGACGGAAAAGCGTTCCATGTGTGCGTGGAGGAACAGGAGCGGAAGCGGTTCTGGGATGGCCATCATGTAGGGGGGCGGGACCCTGAAGAAGCACGGAGCCTTTGTAGGGAGGCCTCAGCAAGGCACCCCTGAGAGAGGACGGCCCAGCGTTGACTCCCAGTTGAGGACAGTTATGCAGATTCCAGGCTGCATAAGCTCCATGGTGGCCTAGCTGCGTTACCCATGCCTGCGTCCTGGCGCCTCACTTGCAAACTCAGCCACATCAGCGCGGGAAGCAGGAGGGGGAGCACCTCGCAGAGCGGCAGGATATTTCAGGAGAGAGTAAAGAGAGAAGGCAGGGGAGGGAGGAGAGACCACAATCAACGTAACTTTCTATTTACAGTAAATGGTTATAGTTGTTCTTTATTGTTAGTTAGTGTTAACCTCTTCCTCGTGCCTGACCTGTGAATTAAACTTAGATCATCCCCCGGCTGTGTATTGAGAAAAAACAGTTATAGCAGACGATGTGGGTACTGGCCTATGGGCTGTCTAGCACCCACTGGGGGTCTTGGCACCTATCCTCCAGGATTGGGGGCTGCTGTGTATCAAAAACACGGCGAATGTTATTCAGTGTAAGGGAATAAAATGCTGACATAGGCTATGACATGATGCACCCTCGAGGACATTCTGGCTCAGTGAAGTCAGCCGTCAGAAAGGACAAAAAAAATTCCGTGGAATTCCACTTACAGGAGGTCCCATTAGAGCTGTCAAATCCAAAAGAGATGGGAAGGCGAAGGGTGGGTGCCTGGGGCTTGGGGGAGGGGACAGCGGAGTGCATTGTTTAATGGGGTGCAGAGGTCAGGTGGGAAGAAGAAGCGTTCTGGCGGTAGATGGTGGTGACGGCTGCACACCAAGCTGAAATATGCGCCCAGTTGCTTGCGGGACGTCTAACTGAAAAATCATGAAGATGGTTAAGTTTACGCTATGTGTATATTTCACACAATTAGAAAAGGCACCCCAGTAAAGTTGGATAAATGTTGACAAATGTGCTACATCCATGTAACTGCCACTCCATTCAAAGTTAGAGCCGCATTGCC'
RG_ARG_MAP: dict[str, str] = {
    "rg-id": "ID",
    "rg-sm": "SM",
    "rg-lb": "LB",
    "rg-pl": "PL",
    "rg-ds": "DS",
    "rg-dt": "DT",
    "rg-pu": "PU",
    "rg-pi": "PI",
    "rg-pg": "PG",
    "rg-cn": "CN",
    "rg-fo": "FO",
    "rg-ks": "KS",
    "rg-pm": "PM",
    "rg-bc": "BC",
}

def collect_rg_metadata(args: argparse.Namespace, option) -> dict[str, str]:
    """Build a SAM @RG dictionary from CLI args."""
    rg_values: dict[str, str] = {}
    for attr, tag in RG_ARG_MAP.items():
        value = getattr(args, attr, None)
        if value is None:
            continue
        rg_values[tag] = str(value)
        option[attr] = str(value)

    if rg_values and "ID" not in rg_values:
        raise ValueError("The --rg-id option is required when any other --rg-* option is supplied.")
    return rg_values
def parse_arguments():
    parser = argparse.ArgumentParser(
        description="VACmap - a long-read aligner for structural variation discovery",
        formatter_class=argparse.RawTextHelpFormatter
    )

    # Required Arguments
    req_group = parser.add_argument_group('Required Arguments')
    req_group.add_argument('-ref', required=True, help="Path to reference FASTA file")
    req_group.add_argument('-read', required=True, nargs='+', action='append', 
                           help="Path to read file(s). Supports wildcards (e.g., *.fastq).")
    req_group.add_argument('-mode', required=True, choices=['H', 'L', 'S', 'R', 'asm'], 
                           help="Alignment mode:\n"
                                " H   : High error rate long reads (PacBio CLR, ONT)\n"
                                " L   : Low error rate long reads (PacBio HiFi)\n"
                                " asm : Assembly mode\n"
                                " S   : Sensitive mode\n"
                                " R   : No prefer closer mode")

    # Output Arguments
    out_group = parser.add_argument_group('Output Arguments')
    out_group.add_argument('-o', default='-', help="Output path (default: stdout). Must end in .sam, .bam, or .sorted.bam if writing to a file. (Note: Output will be sorted if the path ends in .sorted.bam).")
    out_group.add_argument('--force', action='store_true', help="Overwrite output file if it exists.")
    out_group.add_argument('--nowriteindex', action='store_true', help="Do not save the reference index for reuse.")
    
    # Alignment Parameters
    param_group = parser.add_argument_group('Alignment Parameters')
    param_group.add_argument('-t', type=int, default=8, help="Number of threads (default: 8)")
    param_group.add_argument('-k', type=str, default='15', help="Kmer size (default: 15)")
    param_group.add_argument('-w', type=str, default='10', help="Minimizer window size (default: 10)")
    param_group.add_argument('-c', type=int, default=100, help="Top N clusters (default: 100)")
    param_group.add_argument('-maxdivergence', type=float, help="Max sequence divergence (default depends on mode)")
    
    # Penalties
    pen_group = parser.add_argument_group('Penalties')
    pen_group.add_argument('-globalpenalty', type=float, help="Global variation penalty")
    pen_group.add_argument('-localpenalty', type=float, help="Local variation penalty")
    pen_group.add_argument('-globalmaxdiff', type=int, default=50, help="Global max diff (default: 50)")
    pen_group.add_argument('-localmaxdiff', type=int, default=30, help="Local max diff (default: 30)")

    # Flags / Toggles
    flag_group = parser.add_argument_group('Flags')
    flag_group.add_argument('--eqx', action='store_true', help="Output =/X CIGAR operators for sequence match/mismatch.")
    flag_group.add_argument('--MD', action='store_true', help="Output the MD tag.")
    flag_group.add_argument('--cs', nargs='?', const='short', default=None, help="Output the cs tag. Usage: --cs or --cs=long")
    flag_group.add_argument('--L', action='store_true', help="Write CIGAR with >65535 operators at the CG tag (cigar2cg).")
    flag_group.add_argument('--markunbalancetra', action='store_true', help="Set MAPQ of unbalanced translocation subalignment to 1.")
    flag_group.add_argument('--nodiscard', action='store_true', help="Keep subalignments where surrounding indel sizes are similar.")
    flag_group.add_argument('--copycomments', action='store_true', help="Copy FASTA/Q comments to output.")
    flag_group.add_argument('--H', action='store_true', help="Use hard-clipping instead of soft-clipping.")
    flag_group.add_argument('--fakecigar', action='store_true', help="Use approximate CIGAR in the SA tag.")
    flag_group.add_argument('--Q', action='store_true', help="Ignore base quality in the input file.")
    flag_group.add_argument('--debug', action='store_true', help="Enable debug mode.")
    
    # Assembly specific
    flag_group.add_argument('-workdir', help="Working directory (Required if mode is 'asm')")
    

    rg_group = parser.add_argument_group("Read-group (RG) metadata")
    rg_group.add_argument("--rg-id", dest="rg-id", metavar="STRING",
                          help="Adds RG:Z:<string> to all alignments (required when using other --rg-* flags).")
    rg_group.add_argument("--rg-sm", dest="rg-sm", metavar="STRING", help="RG header: Sample (SM).")
    rg_group.add_argument("--rg-lb", dest="rg-lb", metavar="STRING", help="RG header: Library (LB).")
    rg_group.add_argument("--rg-pl", dest="rg-pl", metavar="STRING", help="RG header: Platform (PL).")
    rg_group.add_argument("--rg-ds", dest="rg-ds", metavar="STRING", help="RG header: Description (DS).")
    rg_group.add_argument("--rg-dt", dest="rg-dt", metavar="YYYY-MM-DD", help="RG header: Date (DT).")
    rg_group.add_argument("--rg-pu", dest="rg-pu", metavar="STRING", help="RG header: Platform unit (PU).")
    rg_group.add_argument("--rg-pi", dest="rg-pi", metavar="INT", help="RG header: Median insert size (PI).")
    rg_group.add_argument("--rg-pg", dest="rg-pg", metavar="STRING", help="RG header: Programs (PG).")
    rg_group.add_argument("--rg-cn", dest="rg-cn", metavar="STRING", help="RG header: Sequencing center (CN).")
    rg_group.add_argument("--rg-fo", dest="rg-fo", metavar="STRING", help="RG header: Flow order (FO).")
    rg_group.add_argument("--rg-ks", dest="rg-ks", metavar="STRING", help="RG header: Key sequence (KS).")
    rg_group.add_argument("--rg-pm", dest="rg-pm", metavar="STRING",
                          help="RG header: Platform model (PM).")
    rg_group.add_argument("--rg-bc", dest="rg-bc", metavar="STRING",
                          help="RG header: Barcode sequence (BC).")
    
    return parser.parse_known_args()

def main():
    args, unknown = parse_arguments()
    
    raw_read_list = []
    # Flatten the list of lists
    for sublist in args.read:
        raw_read_list.extend(sublist)

    final_read_list = []
    for path_str in raw_read_list:
        # Check if it contains glob characters (*, ?, [])
        if any(char in path_str for char in "*?[]"):
            # Expand using glob
            expanded_files = glob.glob(path_str)
            if not expanded_files:
                # Warn if pattern matched nothing, but keep it (it will likely error later as file not found)
                logging.warning(f"Wildcard pattern matched no files: {path_str}")
                final_read_list.append(path_str)
            else:
                final_read_list.extend(expanded_files)
        else:
            final_read_list.append(path_str)
    
    pdict = {}
    
    # Required
    pdict['ref'] = args.ref
    pdict['read'] = final_read_list
    pdict['mode'] = args.mode
    pdict['o'] = args.o
    
    # Basic Config
    pdict['t'] = args.t
    pdict['k'] = args.k
    pdict['w'] = args.w
    pdict['c'] = args.c
    
    # Flags
    pdict['eqx'] = args.eqx
    pdict['md'] = args.MD # Note capitalization mapping
    pdict['cigar2cg'] = args.L
    pdict['copycomments'] = args.copycomments
    pdict['H'] = args.H
    pdict['fakecigar'] = args.fakecigar
    pdict['Q'] = args.Q
    pdict['nowriteindex'] = args.nowriteindex
    pdict['debug'] = args.debug
    
    if args.force:
        pdict['force'] = True

    # Handle CS tag logic
    pdict['shortcs'] = True # Default
    if args.cs is not None:
        pdict['cs'] = True
        if args.cs == 'long':
            pdict['shortcs'] = False
        else:
            pdict['shortcs'] = True

    rg_metadata = collect_rg_metadata(args, pdict)

    

    # Logic: Validate Output Path
    if pdict['o'] != "-":
        if not (pdict['o'].endswith(".bam") or pdict['o'].endswith(".sam")):
             raise ValueError("Output path must end with .sam, .bam, .sorted.bam, or be '-' for stdout.")
        if os.path.isfile(pdict['o']) and 'force' not in pdict:
            raise ValueError("Output file exists. Use --force to ignore.")

    # Logic: Validate Files
    if not os.path.isfile(pdict['ref']):
        logging.error(f"Reference file not found: {pdict['ref']}")
        sys.exit(1)
    
    for filepath in pdict['read']:
        if not os.path.isfile(filepath):
            logging.error(f"Read file not found: {filepath}")
            sys.exit(1)

    # Logic: Mode Specific Settings
    if pdict['mode'] == 'asm':
        pdict['eqx'] = True
        if not args.workdir:
             logging.error('workdir not provided! -workdir /path/to/workdir')
             sys.exit(1)
        pdict['workdir'] = str(args.workdir)
        if not os.path.isdir(pdict['workdir']):
            logging.info(f"Creating workdir: {pdict['workdir']}")
            os.mkdir(pdict['workdir'])
        if(pdict['workdir'].endswith('/') == False):
            pdict['workdir'] += '/'

    pdict['local_kmersize'] = 9

    # Defaults based on Mode (H/L)
    # Renamed golbal -> global
    if pdict['mode'] == 'L':
        default_local_skip = 59.
        default_global_skip = 40.
        default_divergence = 0.1
    elif pdict['mode'] == 'H':
        default_local_skip = 40.
        default_global_skip = 40.
        default_divergence = 0.2
    else:
        default_local_skip = 30.
        default_global_skip = 30.
        default_divergence = 0.5

    # Apply penalties
    pdict['local_skipcost'] = args.localpenalty if args.localpenalty is not None else default_local_skip
    pdict['golbal_skipcost'] = args.globalpenalty if args.globalpenalty is not None else default_global_skip
    
    # Apply divergence
    pdict['maxdivergence'] = args.maxdivergence if args.maxdivergence is not None else default_divergence

    # Apply max diffs
    pdict['golbal_maxdiff'] = args.globalmaxdiff
    pdict['local_maxdiff'] = args.localmaxdiff

    # Logic: Mark Unbalanced / No Discard
    if args.markunbalancetra:
        pdict['markunbalancetra'] = True
    else:
        pdict['markunbalancetra'] = (pdict['mode'] in ('L', 'H'))

    if args.nodiscard:
        pdict['nodiscard'] = True
    else:
        # Original logic reversed: if L/H, nodiscard is False, else True. 
        # But if user set --nodiscard, it's True.
        pdict['nodiscard'] = (pdict['mode'] not in ('L', 'H'))

    # Logging Configuration Summary
    logging.info(f"CMD: {' '.join(sys.argv)}")
    logging.info(f"Mode: {pdict['mode']}")
    logging.info(f"Threads: {pdict['t']}")
    logging.info(f"Kmer size: {pdict['k']}, Window: {pdict['w']}")
    logging.info(f"Max divergence: {pdict['maxdivergence']}")
    logging.info(f"Global penalty: {pdict['golbal_skipcost']}")
    logging.info(f"Local penalty: {pdict['local_skipcost']}")

    # --- Import Module based on Mode ---
    H_bool = False
    mammap = None
    
    if pdict['mode'] == 'H':
        import vacmap.mammap_clrnano as mammap
    elif pdict['mode'] == 'L':
        import vacmap.mammap_ccs as mammap
    elif pdict['mode'] == 'S':
        H_bool = True
        import vacmap.mammap_sensitive as mammap
    elif pdict['mode'] == 'R':
        import vacmap.mammap_noprefercloser as mammap
    elif pdict['mode'] == 'asm':
        import vacmap.mammap_asm as mammap

    # --- Index Building/Loading ---
    refpath = pdict['ref']
    if not pdict['nowriteindex']:
        index_name = f"{refpath}.w{pdict['w']}_k{pdict['k']}.mmi"
        if not os.path.isfile(index_name):
            if not refpath.endswith('mmi'):
                logging.info("Using minimap2 to build reference index")
                import subprocess
                cmd = ["minimap2", "-d", index_name, refpath, "-w", str(pdict['w']), "-k", str(pdict['k']), "-I", "1000G"]
                subprocess.run(cmd)
                refpath = index_name
                if not os.path.isfile(refpath):
                    logging.error("Failed to build reference index")
                    sys.exit(1)
        else:
            refpath = index_name
    
    pdict['ref'] = refpath # Update ref path to index if applicable

    # Load Index
    logging.info(f"Loading index: {refpath}")
    index_object = mp.Aligner(refpath, w=int(pdict['w']), k=int(pdict['k']))
    if not index_object:
        raise Exception("ERROR: failed to load index")

    # Setup Contig Dictionaries
    contig2start = NumbaDict()
    contig2seq = NumbaDict()
    index2contig = NumbaList()
    contig2iloc = dict()
    header = {'HD': {'VN': '1.0'}, 'SQ': []}
    if rg_metadata:
        header["RG"] = [rg_metadata]
    
    iloc = -1
    for item in index_object.seq_offset:
        iloc += 1
        name = item[0].decode()
        contig2start[name] = item[2]
        # Assuming index_object.seq returns bytes or string, converting to string and upper
        seq_content = index_object.seq(name).upper()
        contig2seq[name] = seq_content
        index2contig.append(name)
        header['SQ'].append({'LN': len(seq_content), 'SN': name})
        contig2iloc[name] = iloc

    
    header['PG'] = [{'ID': 'VACmap', 'PN': 'VACmap', 'VN': '1.0.2', 'CL': ' '.join(sys.argv)}]

    # --- Test Alignment (Original Logic Preservation) ---
    try:
        # Note: 'global_skipcost' key logic might need to be 'golbal_skipcost' if 
        # internal mammap modules use the typo. We copy just in case.
        # Since I cannot see the inside of mammap, I will inject the typo back 
        # into pdict for compatibility with the internal module.
        pdict['golbal_skipcost'] = pdict['global_skipcost']
        pdict['local_skipcost'] = pdict['local_skipcost']

        mammap.get_readmap_DP_test(
            'hhk', 
            str(Seq(TEST_SEQ).reverse_complement()), 
            contig2start, contig2seq, index_object, index2contig, pdict, 
            hastra=True, refine=True, debug=False, H=H_bool
        )
    except Exception as error:
        pass

    # --- Multiprocessing Setup ---
    max_worker = pdict['t']
    manager = multiprocessing.Manager()
    
    if pdict['mode'] == 'asm':
        raw_queue = manager.Queue(maxsize=1)
        cooked_queue = manager.Queue(maxsize=1)
        writer_func = output_functions.sam_bam_writer_asm
    else:
        raw_queue = manager.Queue(maxsize=max_worker * 200)
        cooked_queue = manager.Queue(maxsize=max_worker * 200)
        writer_func = output_functions.sam_bam_writer

    write_process = multiprocessing.Process(target=writer_func, args=(cooked_queue, header, pdict['o']))
    write_process.start()

    process_list = []
    
    # Determine worker target function
    if pdict['mode'] == 'asm':
        target_func = mammap.assembly_get_list_of_readmap_stdout_comments if pdict['copycomments'] else mammap.assembly_get_list_of_readmap_stdout
    else:
        target_func = mammap.get_list_of_readmap_stdout_comments if pdict['copycomments'] else mammap.get_list_of_readmap_stdout

    for _ in range(max_worker):
        p = multiprocessing.Process(
            target=target_func, 
            args=(raw_queue, cooked_queue, index_object, contig2seq, True, pdict['H'], header, pdict)
        )
        p.start()
        process_list.append(p)

    # --- Consolidated File Reading Loop ---
    st = time.time()
    tmp_st = st
    count = 0
    tmp_count = 0
    
    unique_set = set()

    for fastapath in pdict['read']:
        logging.info(f"Reading {fastapath}")
        
        # Check file type
        is_bam = fastapath.endswith('.bam')
        
        iterator = None
        if is_bam:
            # BAM processing
            bamfile = pysam.AlignmentFile(fastapath, check_sq=False)
            iterator = bamfile
        else:
            # FASTQ/FASTA processing
            # Check if we need comments
            read_comments = pdict['copycomments'] 
            iterator = mp.fastx_read(fastapath, read_comment=pdict['copycomments'])

        for rec in iterator:
            name = ""
            seq = ""
            qual = None
            comment = None
            
            # --- Extract Data based on Source ---
            try:
                if is_bam:
                    name = rec.query_name
                    if hash(name) in unique_set: continue
                    seq = rec.query_sequence.upper()
                    if rec.is_reverse:
                        seq = str(Seq(seq).reverse_complement())
                    qual = None 
                else:
                    # mp.fastx_read returns tuple
                    name = rec[0]
                    if hash(name) in unique_set: continue
                    seq = rec[1].upper()
                    
                    if(pdict['Q'] == False and len(rec) > 2): 
                        qual = rec[2]
                    if(len(rec) > 3): 
                        comment = rec[3]
            except Exception as e:
                logging.error('Failed to decode one read sequence, continue anyway.')
                continue

            # Deduplication
            unique_set.add(hash(name))
            count += 1
            
            # --- Put to Queue ---
            # Format: (name, seq, qual) or (name, seq, qual, comment)
            if pdict['copycomments'] and comment is not None:
                raw_queue.put((name, seq, qual, comment))
            else:
                raw_queue.put((name, seq, qual))

            # --- Reporting ---
            if count % 100000 == 0:
                c_time = time.time()
                tmptime = c_time - tmp_st
                t_time = c_time - st
                
                # Prevent division by zero
                tmptime = max(tmptime, 0.001)
                t_time = max(t_time, 0.001)
                
                msg = (f"{round((count - tmp_count) / tmptime)} / sec in the last "
                       f"{max(round(tmptime/60), 1)} minutes, "
                       f"{round(count / t_time)} / sec AVG. "
                       f"{count} sequences processed.")
                logging.info(msg)
                
                tmp_st = c_time
                tmp_count = count

    # --- Cleanup ---
    # Signal workers to stop
    for _ in range(max_worker):
        raw_queue.put(0) # Sending 0 (or some sentinel) to stop workers. Original code used 'i' range(n) but implied int.
    
    for p in process_list:
        p.join()

    # Calculate final stats
    c_time = time.time()
    t_time = max(c_time - st, 0.001)
    
    # Signal writer to stop
    cooked_queue.put(0)
    write_process.join()

    hours = int(t_time // 3600)
    minutes = int((t_time % 3600) // 60)
    seconds = int(t_time % 60)
    
    logging.info(f"User time (h:m:s): {hours}:{minutes}:{seconds} "
                 f"{round(count / t_time)} / sec AVG. "
                 f"{count} sequences processed.")

if __name__ == "__main__":
    main()
